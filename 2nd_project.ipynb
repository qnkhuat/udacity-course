{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2nd project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "smDySyhDUG8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train on multilple machines\n",
        "# Move all gradient to a machne\n",
        "# aggregate the gradient on that machine and then get it back to our machine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13J1tYr-xajD",
        "colab_type": "code",
        "outputId": "bca85506-1727-40bc-fda8-7dc9a37d06dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1312
        }
      },
      "source": [
        "! pip install syft"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting syft\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/f4/ed95c5f6664d37a70c345f41e1bdceadc5efb2f478e2c6cd4de34f3a709a/syft-0.1.18-py3-none-any.whl (186kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.6)\n",
            "Collecting tf-encrypted>=0.5.4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/ce/da9916e7e78f736894b15538b702c0b213fd5d60a7fd6e481d74033a90c0/tf_encrypted-0.5.6-py3-none-manylinux1_x86_64.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 33.0MB/s \n",
            "\u001b[?25hCollecting flask-socketio (from syft)\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/68/fe4806d3a0a5909d274367eb9b3b87262906c1515024f46c2443a36a0c82/Flask_SocketIO-4.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tblib in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from syft) (0.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.6/dist-packages (from syft) (1.0.3)\n",
            "Collecting websockets>=7.0 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/71/8bfa882b9c502c36e5c9ef6732969533670d2b039cbf95a82ced8f762b80/websockets-7.0-cp36-cp36m-manylinux1_x86_64.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 26.0MB/s \n",
            "\u001b[?25hCollecting lz4 (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl (385kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 46.6MB/s \n",
            "\u001b[?25hCollecting zstd (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/27/1ea8086d37424e83ab692015cc8dd7d5e37cf791e339633a40dc828dfb74/zstd-1.4.0.0.tar.gz (450kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 49.9MB/s \n",
            "\u001b[?25hCollecting websocket-client (from syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 42.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.14.0rc1)\n",
            "Collecting pyyaml>=5.1 (from tf-encrypted>=0.5.4->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.16.4)\n",
            "Collecting python-socketio>=2.1.0 (from flask-socketio->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/e0/a875abbb0f9d70c6e2ec2019ca9e3893377cef5deca6225ead3497000152/python_socketio-4.1.0-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 22.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->syft) (0.21.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (2.10.1)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from Flask->syft) (0.15.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0rc1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.13.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.11.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Collecting python-engineio>=3.8.0 (from python-socketio>=2.1.0->flask-socketio->syft)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/d3/de0fa7ebebd054de308e6ac397bf9e11ea42924795a38005a21ea001b114/python_engineio-3.8.1-py2.py3-none-any.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->syft) (0.13.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->syft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->Flask->syft) (1.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Building wheels for collected packages: zstd, pyyaml\n",
            "  Building wheel for zstd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/9a/f4/3105b5209674ac77fcca7fede95184c62a95df0196888e0e76\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
            "Successfully built zstd pyyaml\n",
            "Installing collected packages: pyyaml, tf-encrypted, python-engineio, python-socketio, flask-socketio, websockets, lz4, zstd, websocket-client, syft\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed flask-socketio-4.1.0 lz4-2.1.10 python-engineio-3.8.1 python-socketio-4.1.0 pyyaml-5.1.1 syft-0.1.18 tf-encrypted-0.5.6 websocket-client-0.56.0 websockets-7.0 zstd-1.4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSLfaKaGUG8j",
        "colab_type": "code",
        "outputId": "510abc67-42dd-47de-c7b1-01dce1533f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "import torch as th\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import syft as sy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0623 14:38:37.769871 140553504298880 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0-rc1.so'\n",
            "W0623 14:38:37.794982 140553504298880 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaaAM6cjUG8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hook = sy.TorchHook(th)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abglNrlvUG8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "#     datasets.CIFAR10('../data', train=True, download=True,\n",
        "#                    transform=transforms.Compose([\n",
        "#                        transforms.ToTensor(),\n",
        "#                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "#                    ]))\n",
        "#     .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "#     batch_size=args.batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUxW8BtUUG8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data_loaders (num_training_loaders):\n",
        "\n",
        "    # Define a transform to normalize the data\n",
        "    transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    # Download and load the training data\n",
        "    mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    # Set up the trainloaders list\n",
        "    trainloaders = []\n",
        "    data_per_set = int(len(mnist_trainset) / num_training_loaders)\n",
        "    for i in range(num_training_loaders):\n",
        "        train_indices = range(i*data_per_set,i*data_per_set+data_per_set)\n",
        "        trainloaders.append(torch.utils.data.DataLoader(mnist_trainset, \n",
        "                                                         batch_size=32, \n",
        "                                                         sampler=SubsetRandomSampler(train_indices)))\n",
        "\n",
        "    # Download and load the test data\n",
        "    mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    mnist_test_train = Subset(mnist_testset,range(9000))\n",
        "    mnist_test_test = Subset(mnist_testset,range(9000,10001))\n",
        "    # Set up the testloader\n",
        "    # Use for train student model\n",
        "    testtrainloader = torch.utils.data.DataLoader(mnist_test_train, batch_size=32, shuffle=False)\n",
        "    testtrain_targets = mnist_testset.test_labels[:9000]\n",
        "    \n",
        "    # Use for evaluate the student model\n",
        "    testtestloader = torch.utils.data.DataLoader(mnist_test_test, batch_size=1000, shuffle=False)\n",
        "    testtest_targets = mnist_testset.test_labels[9000:]\n",
        "\n",
        "    return trainloaders, testtrainloader,testtestloader, testtrain_targets, testtest_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsSD6o9rUG8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_federated_data_loaders(num_training_loaders,workers):\n",
        "\n",
        "    # Define a transform to normalize the data\n",
        "    transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    # Download and load the training data\n",
        "    mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    # Set up the trainloaders list\n",
        "    trainloaders = []\n",
        "    data_per_set = int(len(mnist_trainset) / num_training_loaders)\n",
        "    for i in range(num_training_loaders):\n",
        "        train_indices = range(i*data_per_set,i*data_per_set+data_per_set)\n",
        "#         trainloaders.append(torch.utils.data.DataLoader(mnist_trainset, \n",
        "#                                                          batch_size=32, \n",
        "#                                                          sampler=SubsetRandomSampler(train_indices)))\n",
        "        trainloaders.append(sy.FederatedDataLoader(mnist_trainset, \n",
        "                                                         batch_size=32, \n",
        "                                                         sampler=SubsetRandomSampler(train_indices))).federate(workers)\n",
        "\n",
        "    # Download and load the test data\n",
        "    mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    # Set up the testloader\n",
        "    \n",
        "    testtrainloader = sy.FederatedDataLoader(mnist_testset, batch_size=1000, shuffle=False,\n",
        "                                                 sampler=SubsetRandomSampler(range(9000))).federate(workers)\n",
        "\n",
        "    testtestloader = sy.FederatedDataLoader(mnist_testset, batch_size=1000, shuffle=False,\n",
        "                                                 sampler=SubsetRandomSampler(range(9000,10001))).federate(workers)\n",
        "    test_targets = mnist_testset.test_labels\n",
        "\n",
        "    return trainloaders, testtrainloader,testtestloader, test_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZMguw1VUG8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bob, alice, secure = sy.VirtualWorker(hook,id='bob'), sy.VirtualWorker(hook,id='alice'), sy.VirtualWorker(hook,id='secure')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlbr340fUG80",
        "colab_type": "code",
        "outputId": "4b50694c-170d-4e84-cae1-781e39d4a27d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "train_loader, test_loader = create_federated_data_loaders(2,)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ea951c49e29d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_federated_data_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: create_federated_data_loaders() missing 1 required positional argument: 'workers'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD5fMGidUG82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784,256)\n",
        "        self.fc2 = nn.Linear(256,64)\n",
        "        self.fc3 = nn.Linear(64,10)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.log_softmax(self.fc3(x),dim=1)\n",
        "    \n",
        "        return x\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvpxd4HkUG84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a model and send it to both machines\n",
        "# compute anything on those 2 machines \n",
        "# at the end send those 2 model to secure workder\n",
        "# averange the weights and bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQy2tyTOUG86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, machines, secure, dataset,epochs=4):\n",
        "    criterion = nn.NLLLoss() \n",
        "    models = [] # store the model on all machines\n",
        "    for machine in machines : \n",
        "        model_ = model.copy().send(machine) # model on the remote machine\n",
        "        optimizer_ = optim.Adam(model_.parameters(),lr=1e-3)\n",
        "        for i in range(epochs):\n",
        "            running_loss = 0\n",
        "            for data, target in dataset:\n",
        "                _data = data.send(machine)\n",
        "                _target = target.send(machine)\n",
        "\n",
        "                optimizer_.zero_grad()\n",
        "                pred = model_(_data)\n",
        "                loss = criterion(pred,_target)\n",
        "                loss.backward()\n",
        "                optimizer_.step()\n",
        "\n",
        "#                 model = model.get()\n",
        "                running_loss +=loss.get()\n",
        "            \n",
        "            print(f\"Epoch {i} : {running_loss/len(dataset)}\")\n",
        "        model_.move(secure) # send the secure worker\n",
        "        models.append(model_)\n",
        "    return models # later to aggregare gradieint"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BFoB1lkUG89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG0u9M2gUG8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = train(model,(bob,alice),secure,train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w24H_mlfUG9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# aggregate the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tde5Okc6UG9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with th.no_grad():\n",
        "    for model_layer,bob_layer,alice_layer in zip(model.children(),models[0].children(),models[1].children()):\n",
        "        model_layer.weight.set_(((bob_layer.weight.data + alice_layer.weight.data)/2).get())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWB26c0FUG9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntU_La-GUG9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}